{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Process# this is used for multithreading\n",
    "import multiprocessing\n",
    "import codecs# this is used for file operations \n",
    "import random as r\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for generating unigram and bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bigram\n",
    "# chrs = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "# single_gram = []\n",
    "# bi_gram = []\n",
    "# three_gram = []\n",
    "\n",
    "\n",
    "# for ch in chrs:\n",
    "#     for ch1 in chrs:\n",
    "#         single_gram.append(ch+ch1)\n",
    "# print(single_gram)\n",
    "# print(\"Length for single of words: \",len(single_gram))\n",
    "\n",
    "# single_gram.append(\"??\")\n",
    "\n",
    "# for wrd in single_gram:\n",
    "#     for wrd1 in single_gram:\n",
    "#         bi_gram.append(wrd+\" \"+wrd1)\n",
    "# print(\"Length for pair of words: \",len(bi_gram))\n",
    "        \n",
    "        \n",
    "# for wrd in single_gram:\n",
    "#     for wrd1 in single_gram:\n",
    "#         for wrd2 in single_gram:\n",
    "#             three_gram.append(wrd+\" \"+wrd1+\" \"+wrd2)\n",
    "\n",
    "# print(\"Length for three pair of words: \",len(three_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_gram.extend(bi_gram)\n",
    "# len(single_gram)\n",
    "# bi_gram = single_gram\n",
    "# bi_gram.extend([\"??\"])\n",
    "# bi_gram_dict = {}\n",
    "# i=0\n",
    "# bi_gram_dict = { s:i for i,s in enumerate(bi_gram) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtfiles = []\n",
    "# [txtfiles.append(f) if f.endswith(\"txt\") else None for f in os.listdir(destination) ]\n",
    "# print(\"Text file List generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #logic to create all single gram and bigram in the file to process further in one row for sparse matrix\n",
    "# import time\n",
    "# import pickle\n",
    "# from scipy.sparse import csr_matrix\n",
    "# import sys\n",
    "# import gc\n",
    "# start = time.time()\n",
    "# def incrarr(arr, ind):\n",
    "#     arr[ind]+=1\n",
    "    \n",
    "# def create_matrix(txtfiles, no):\n",
    "#     sparse_row = []\n",
    "#     sparse_col = []\n",
    "#     sparse_data = []\n",
    "#     for findex, file in enumerate(txtfiles):\n",
    "#         with open(destination+\"/\"+file,\"r\") as fp:\n",
    "#             big_bigram_row = []\n",
    "#             for i, line in enumerate(fp):\n",
    "#                 single = line.split() #this will produce single gram\n",
    "#                 double = [single[j-1]+\" \"+single[j] for j, s in enumerate(single[1:])]\n",
    "#                 single.extend(double)\n",
    "#                 big_bigram_row.extend(single)\n",
    "# #             big_bigram_row = list(set(big_bigram_row))\n",
    "#             temp_row = np.zeros(len(bi_gram))\n",
    "#             [incrarr(temp_row, bi_gram_dict[s.lower()]) for ind, s in enumerate(big_bigram_row)] # fast but not a good practise solution\n",
    "#             col_no, ele = [], []\n",
    "#             dt = []\n",
    "#             for tind, ele in enumerate(temp_row):\n",
    "#                 if ele>0:\n",
    "#                     col_no.append(tind)\n",
    "#                     dt.append(ele)\n",
    "#             ro_no = np.ones(len(dt))*findex\n",
    "#         sparse_row.extend(ro_no)\n",
    "#         sparse_col.extend(col_no)\n",
    "#         sparse_data.extend(dt)\n",
    "#         sys.stdout.write('\\r'+str(findex))\n",
    "#         gc.collect()\n",
    "#     flnm = \"Vikalp/opt/\"+str(findex)+\"_\"+str(no)+\"_spm.pickle\"\n",
    "#     file = open(flnm, \"wb\")\n",
    "#     ok = csr_matrix((sparse_data, (sparse_row, sparse_col)), shape=(findex+1, len(bi_gram_dict)+1))\n",
    "#     pickle.dump(ok, file)\n",
    "#     file.close()\n",
    "#     del ok\n",
    "#     gc.collect()\n",
    "\n",
    "# print(\"Execution time for the snippet: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start = time.time()\n",
    "# for no, textfiles in enumerate(txtfile_matrix):\n",
    "#     create_matrix(textfiles, no)\n",
    "#     gc.collect()\n",
    "#     print()\n",
    "#     print(no, \"Batch complete\")\n",
    "# print(\"Execution time for the snippet: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_mat_files = ['999_0_spm.pickle', '999_1_spm.pickle',\n",
    "#        '999_2_spm.pickle', '999_3_spm.pickle', '999_4_spm.pickle',\n",
    "#        '999_5_spm.pickle', '999_6_spm.pickle', '999_7_spm.pickle',\n",
    "#        '999_8_spm.pickle', '999_9_spm.pickle', '867_10_spm.pickle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have executed this above code in `Vikalp_.ipynb` now using pickle file for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n",
      "csr_mat is ready!!!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "combine_file_name  = \"Vikalp/opt/csr_mat_combine.pickle\"\n",
    "if os.path.exists(combine_file_name):\n",
    "    print(\"File exists!\")\n",
    "    fl = open(combine_file_name, \"rb\")\n",
    "    csr_mat = pickle.load(fl)\n",
    "    fl.close()\n",
    "    print(\"csr_mat is ready!!!\")\n",
    "else:\n",
    "#     sparse_mat_files = []\n",
    "#     [sparse_mat_files.append(f) if \"pickle\" in f else None for f in os.listdir('Vikalp/opt') ]\n",
    "#     print(\"Text file List generated\")\n",
    "    print(sparse_mat_files)\n",
    "    f  = open(\"Vikalp/opt/\"+sparse_mat_files[0], \"rb\")\n",
    "    csr_mat = pickle.load(f)\n",
    "    print(csr_mat.shape)\n",
    "\n",
    "    for i in sparse_mat_files[1:]:\n",
    "        f  = open(\"Vikalp/opt/\"+i, \"rb\")\n",
    "        csr_mat1 = pickle.load(f)\n",
    "        csr_mat = vstack((csr_mat, csr_mat1))\n",
    "    del csr_mat1\n",
    "    print(csr_mat.shape)\n",
    "    file = open(combine_file_name, \"wb\")\n",
    "    pickle.dump(csr_mat, file)\n",
    "    file.close()\n",
    "    print(\"Pickle file created successfully!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle file to get target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n",
      "data_size_byte is ready!!!\n"
     ]
    }
   ],
   "source": [
    "combine_file_name  = \"Vikalp/opt/data_size_byte.pickle\"\n",
    "if os.path.exists(combine_file_name):\n",
    "    print(\"File exists!\")\n",
    "    fl = open(combine_file_name, \"rb\")\n",
    "    data_size_byte = pickle.load(fl)\n",
    "    fl.close()\n",
    "    print(\"data_size_byte is ready!!!\")\n",
    "else:\n",
    "    file = open(combine_file_name, \"wb\")\n",
    "    pickle.dump(data_size_byte, file)\n",
    "    file.close()\n",
    "    print(\"Pickle file created successfully!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>HJQiyIRqr6FPeBcoaEsk</td>\n",
       "      <td>6.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>G8hm6UqIKBQWlMpeTScb</td>\n",
       "      <td>0.785156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6mUHQtCBjzWA0fGIEnP7</td>\n",
       "      <td>7.441406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9gMZ6wVFX7KvHN3y8LoG</td>\n",
       "      <td>7.136719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>hqzvHQ4UBkTPinujM1RC</td>\n",
       "      <td>2.261719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                    ID      size\n",
       "0      3  HJQiyIRqr6FPeBcoaEsk  6.703125\n",
       "1      1  G8hm6UqIKBQWlMpeTScb  0.785156\n",
       "2      2  6mUHQtCBjzWA0fGIEnP7  7.441406\n",
       "3      2  9gMZ6wVFX7KvHN3y8LoG  7.136719\n",
       "4      6  hqzvHQ4UBkTPinujM1RC  2.261719"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size_byte.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle file to get the textfiles list for calculating pixel intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file existed\n",
      "Operation completed\n"
     ]
    }
   ],
   "source": [
    "filename=\"txtfiles\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    txtfiles = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(txtfiles, file)\n",
    "    file.close()\n",
    "print(\"Operation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the target from `data_size_byte` which is used to generate target and `txtfiles` which is used to generate `csr_mat` and `pixel_intensities` in future. It must be in proper order than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, '9gMZ6wVFX7KvHN3y8LoG', 7.13671875], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size_byte.values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9gMZ6wVFX7KvHN3y8LoG.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above both must point to the same file for further use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target from `data_size_bytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename=\"data_y\"\n",
    "# if os.path.exists(filename):\n",
    "#     print(\"pickle file existed\")\n",
    "#     file = open(filename, \"rb\")\n",
    "#     data_y = pickle.load(file)\n",
    "#     file.close()\n",
    "# else:\n",
    "#     print(\"writting in a pickle file.\")\n",
    "#     file = open(filename, \"wb\")\n",
    "#     pickle.dump(data_y, file)\n",
    "#     file.close()\n",
    "# print(\"Operation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data_size_byte['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching perfectly!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for generating pixel intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=VLQTRlLGz5Y\n",
    "# Only starting 800 pixels are enough they said\n",
    "import sys\n",
    "from array import array\n",
    "def get_intens(asm_location, i):\n",
    "    if i%500==0: sys.stdout.write(\"\\r \"+str(i))\n",
    "    f = open(asm_location, \"rb\")\n",
    "    ln = os.path.getsize(asm_location) #it is the length\n",
    "    width = int(ln**0.5) #half will be width\n",
    "    rem = ln%width #find remainder \n",
    "    a = array(\"B\") #creating a byte array\n",
    "    a.fromfile(f, ln-rem)\n",
    "    f.close()\n",
    "#     return a[:2500]\n",
    "    return a[:7000]\n",
    "    # 0 to 799 in which values varies from 0 to 256 which represents a byte we read\n",
    "    # print(a)\n",
    "\n",
    "    # Only starting 800 pixels are enough they said\n",
    "    # we are not creating images we are just keeping the 800 pixels\n",
    "    # g = np.reshape(a, (int(len(a)/width), width))\n",
    "    # g = np.uint8(g)\n",
    "    # imsave(loc+\"/ok.png\", g)\n",
    "# loc=\"D:/Work/ML/API/MNB/17 - MICROSOFT MALWARE(REMAINING)/sample_asm\"\n",
    "# asm_location=loc+\"/Al6GOBJ2KvPCxo3b7jiN.asm\"    \n",
    "# get_intens(asm_location, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_intensities = [get_intens((source+\"/\"+fl).replace(\".txt\", \".asm\"), ind) for ind, fl in enumerate(txtfiles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to fetch pixel intensities from pickle file 7000 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file existed\n",
      "Operation completed\n"
     ]
    }
   ],
   "source": [
    "filename=\"pixel_intensities7000\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    pixel_intensities = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(pixel_intensities, file)\n",
    "    file.close()\n",
    "print(\"Operation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining both the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import hstack\n",
    "# X = hstack((csr_mat, pd.DataFrame(pixel_intensities)))\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del csr_mat, pixel_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file existed\n",
      "Operation completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10868, 73307)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dont know the reason but ram is taking the load of 31gb currently.\n",
    "filename=\"X7000\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    X = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(X, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garbage collection to remove unnecessary variables from memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n",
      "data_size_byte is ready!!!\n"
     ]
    }
   ],
   "source": [
    "combine_file_name  = \"Vikalp/opt/data_size_byte.pickle\"\n",
    "if os.path.exists(combine_file_name):\n",
    "    print(\"File exists!\")\n",
    "    fl = open(combine_file_name, \"rb\")\n",
    "    data_size_byte = pickle.load(fl)\n",
    "    fl.close()\n",
    "    print(\"data_size_byte is ready!!!\")\n",
    "else:\n",
    "    file = open(combine_file_name, \"wb\")\n",
    "    pickle.dump(data_size_byte, file)\n",
    "    file.close()\n",
    "    print(\"Pickle file created successfully!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data_size_byte['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now train test split individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data_y,stratify=data_y,test_size=0.20, random_state=22)\n",
    "# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)\n",
    "print(\"Split successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X,data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data is compulsory and so much important because there are different values having different scale and we have to finalize them in a single scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing min max scaler because there are less chances of outliers and min max scaler will preserve the distribution. It is least disruptive to the information in the orignal data.<br>\n",
    "Ref Link: https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# X_train = X_train.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "X_cv = min_max_scaler.transform(X_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing dimensionality using selectkbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow random forest classifier is not working as expected because we have to tune the parameters there too so consider ing that we will use selectkbest which will calculate using chi square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selkbest = SelectKBest(chi2, k=7000)\n",
    "important_X_train = selkbest.fit_transform(X_train, y_train)\n",
    "important_X_cv = selkbest.transform(X_cv)\n",
    "important_X_test = selkbest.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6955, 7000)\n",
      "(6955,)\n",
      "(2174, 7000)\n",
      "(2174,)\n",
      "(1739, 7000)\n",
      "(1739,)\n"
     ]
    }
   ],
   "source": [
    "print(important_X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(important_X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(important_X_cv.shape)\n",
    "print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test , X_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the train targets in pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file existed\n",
      "Operation completed\n",
      "pickle file existed\n",
      "Operation completed\n",
      "pickle file existed\n",
      "Operation completed\n",
      "pickle file existed\n",
      "Operation completed\n",
      "pickle file existed\n",
      "Operation completed\n",
      "pickle file existed\n",
      "Operation completed\n"
     ]
    }
   ],
   "source": [
    "# important_X_train\n",
    "filename=\"important_X_train\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    important_X_train = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(important_X_train, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n",
    "\n",
    "\n",
    "#Dont know the reason but ram is taking the load of 31gb currently.\n",
    "filename=\"important_X_cv\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    important_X_cv = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(important_X_cv, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n",
    "\n",
    "\n",
    "#Dont know the reason but ram is taking the load of 31gb currently.\n",
    "filename=\"important_X_test\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    important_X_test = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(important_X_test, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "filename=\"y_train\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    y_train = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(y_train, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n",
    "\n",
    "\n",
    "filename=\"y_test\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    y_test = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(y_test, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n",
    "\n",
    "\n",
    "# y_cv\n",
    "filename=\"y_cv\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    y_cv = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(y_cv, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6955, 7000)\n",
      "(6955,)\n",
      "(2174, 7000)\n",
      "(2174,)\n",
      "(1739, 7000)\n",
      "(1739,)\n"
     ]
    }
   ],
   "source": [
    "print(important_X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(important_X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(important_X_cv.shape)\n",
    "print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying XGBoost using random searchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 556.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 1000, 'subsample': 0.8}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'subsample': [0.4, 0.6, 0.8],\n",
    "        'max_depth': [2, 3, 5],\n",
    "        'n_estimators': [100,500, 1000, 3000, 5000]\n",
    "        }\n",
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8)\n",
    "\n",
    "gs = RandomizedSearchCV(xgbreg,params,n_iter=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "gs.fit(important_X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005212224824322016\n",
      "0.012337174892977646\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = gs.predict_proba(important_X_train)\n",
    "predict_y_test = gs.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=gs.best_estimator_.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=gs.best_estimator_.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We overfitted the data trying with less iteration and less folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 62.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'subsample': [0.4, 0.6, 0.8],\n",
    "        'max_depth': [2, 3, 5],\n",
    "        'n_estimators': [100,500, 1000, 3000, 5000]\n",
    "        }\n",
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8)\n",
    "\n",
    "gs = RandomizedSearchCV(xgbreg,params,n_iter=3, n_jobs=-1, verbose=1, return_train_score =True)\n",
    "\n",
    "gs.fit(important_X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dont know the reason but ram is taking the load of 31gb currently.\n",
    "filename=\"xgbreg\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"pickle file existed\")\n",
    "    file = open(filename, \"rb\")\n",
    "    xgbreg = pickle.load(file)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"writting in a pickle file.\")\n",
    "    file = open(filename, \"wb\")\n",
    "    pickle.dump(xgbreg, file, protocol=4) # protocol 4 to store file larger than 4gb\n",
    "    file.close()\n",
    "print(\"Operation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001950752742856112\n",
      "0.01422571303198303\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = gs.predict_proba(important_X_train)\n",
    "predict_y_test = gs.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=gs.best_estimator_.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=gs.best_estimator_.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs.cv_results_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we have to search using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>cv_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_child_weight  subsample  max_depth  n_estimators  train_loss  cv_loss\n",
       "0                 1          1          1             1           1        1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok = pd.DataFrame([(1, 1, 1, 1, 1, 1),], columns=cl)\n",
    "ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will early stop as soon as we will get our desired result which is loss<=0.01 in train and cv set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          1000    0.001804  0.013595\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          3000     0.00179  0.013581\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          5000    0.001782  0.013622\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          1000    0.001966  0.012439\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          3000     0.00195  0.012436\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          5000    0.001942  0.012431\n"
     ]
    }
   ],
   "source": [
    "min_child_weight= [3, 5]\n",
    "max_depth= [2, 3, 5]\n",
    "n_estimators= [1000, 3000, 5000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw,\n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue execution as connection broke in middle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          1000    0.001451  0.015225\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          3000    0.001451  0.015225\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          5000    0.001451  0.015225\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          1000    0.001587  0.016896\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          3000    0.001588  0.016896\n"
     ]
    }
   ],
   "source": [
    "min_child_weight= [3, 5]\n",
    "max_depth= [2, 3, 5]\n",
    "n_estimators= [1000, 3000, 5000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw,  \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As connection disconnected so starting again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          1000    0.001683  0.024163\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          3000    0.001683  0.024163\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          5000    0.001683  0.024163\n"
     ]
    }
   ],
   "source": [
    "min_child_weight= [3]\n",
    "max_depth= [5]\n",
    "n_estimators= [1000, 3000, 5000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw,  \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2          1000    0.002599  0.018709\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2          3000    0.002599  0.018709\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2          5000    0.002599  0.018709\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          3          1000    0.002783  0.018669\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          3          3000    0.002783  0.018669\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          3          5000    0.002783  0.018669\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          5          1000    0.002869  0.018686\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          5          3000    0.002869  0.018686\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          5          5000    0.002869  0.018686\n"
     ]
    }
   ],
   "source": [
    "min_child_weight= [5]\n",
    "max_depth= [2, 3, 5]\n",
    "n_estimators= [1000, 3000, 5000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw,  \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(xgboostparam.append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(XGBClassifier)\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As model is overfitting reducing the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selkbest = SelectKBest(chi2, k=3000)\n",
    "important_X_train = selkbest.fit_transform(important_X_train, y_train)\n",
    "important_X_cv = selkbest.transform(important_X_cv)\n",
    "important_X_test = selkbest.transform(important_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2           500    0.000946  0.017429\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2          1000    0.000925  0.017406\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2          3000    0.000925  0.017406\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2          5000    0.000925  0.017406\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3           500    0.001012  0.017286\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3          1000    0.001012  0.017285\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3          3000    0.001012  0.017285\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3          5000    0.001012  0.017285\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5           500    0.001073  0.018596\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5          1000    0.001073  0.018596\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5          3000    0.001073  0.018596\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5          5000    0.001073  0.018595\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2           500    0.001429  0.017636\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          1000    0.001429  0.017633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9b6bf45ff285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   n_jobs=-1 )\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mpredict_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpredict_y_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_child_weight= [2, 3, 5]\n",
    "max_depth= [2, 3, 5]\n",
    "n_estimators= [500, 1000, 3000, 5000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw, \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1           500    0.002431  0.021868\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          1000    0.000627  0.020099\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          3000    0.000414  0.019169\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          5000    0.000414  0.019169\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          2           500    0.000511  0.016918\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          2          1000    0.000449  0.016723\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          2          3000    0.000449  0.016723\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          2          5000    0.000449  0.016723\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          3           500    0.000511  0.018022\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          3          1000    0.000498  0.017963\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          3          3000    0.000498  0.017964\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          3          5000    0.000498  0.017967\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          5           500    0.000531  0.017457\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          5          1000    0.000531  0.017453\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          5          3000    0.000531  0.017458\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          5          5000    0.000531  0.017462\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          1           500    0.002742  0.021471\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          1          1000    0.000905  0.019206\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          1          3000    0.000849  0.019032\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          1          5000    0.000849  0.019032\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2           500    0.000946  0.017429\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2          1000    0.000925  0.017406\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2          3000    0.000925  0.017406\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          2          5000    0.000925  0.017406\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3           500    0.001012  0.017286\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3          1000    0.001012  0.017285\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3          3000    0.001012  0.017285\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          3          5000    0.001012  0.017285\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5           500    0.001073  0.018596\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5          1000    0.001073  0.018596\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5          3000    0.001073  0.018596\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 2          5          5000    0.001073  0.018595\n",
      "   min_child_weight  max_depth  n_estimators  train_loss  cv_loss\n",
      "0                 3          1           500    0.003131  0.02192\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          1          1000    0.001306  0.019457\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          1          3000    0.001306  0.019456\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          1          5000    0.001306  0.019456\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2           500    0.001429  0.017636\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          1000    0.001429  0.017633\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          3000    0.001429  0.017633\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          2          5000    0.001429  0.017633\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3           500    0.001553  0.017569\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          1000    0.001553  0.017569\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          3000    0.001553  0.017569\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          3          5000    0.001553  0.017569\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5           500    0.001635  0.019033\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          1000    0.001635  0.019033\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          3000    0.001635  0.019033\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          5000    0.001635  0.019033\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          1           500    0.003957  0.022774\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          1          1000    0.002264  0.020438\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          1          3000    0.002264  0.020438\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          1          5000    0.002264  0.020438\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2           500    0.002498  0.019286\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2          1000    0.002498  0.019286\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2          3000    0.002498  0.019286\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          2          5000    0.002498  0.019286\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          3           500    0.002702  0.018538\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 5          3          1000    0.002702  0.018538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1b1d6381df4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   n_jobs=-1 )\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mpredict_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpredict_y_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_child_weight= [1, 2, 3, 5]\n",
    "max_depth= [1, 2, 3, 5]\n",
    "n_estimators= [500, 1000, 3000, 5000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw, \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_weight= [1, 2, 3, 5]\n",
    "# max_depth= [1, 3, 5]\n",
    "# n_estimators= [ 5000, 7000, 10000, 12000]\n",
    "# cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "# xgboostparam = pd.DataFrame(columns=cl)\n",
    "# for mcw in min_child_weight:\n",
    "#     for md in max_depth:\n",
    "#         for ne in n_estimators:\n",
    "\n",
    "#             xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "#                                    min_child_weight=mcw, \n",
    "#                                    max_depth=md, \n",
    "#                                    n_estimators=ne,\n",
    "#                                   n_jobs=-1 )\n",
    "#             xgbreg.fit(important_X_train, y_train)\n",
    "#             predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "#             predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "#             train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "#             cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "#             ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "#             print(ok)\n",
    "#             xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# # xgboostparam                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the variables further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          5000    0.000416  0.020178\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          7000    0.000416  0.020177\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.000416  0.020177\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         12000    0.000416  0.020177\n"
     ]
    }
   ],
   "source": [
    "min_child_weight= [1, 2, 3, 5]\n",
    "max_depth= [1, 3, 5]\n",
    "n_estimators= [ 5000, 7000, 10000, 12000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw, \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selkbest = SelectKBest(chi2, k=2000)\n",
    "important_X_train = selkbest.fit_transform(important_X_train, y_train)\n",
    "important_X_cv = selkbest.transform(important_X_cv)\n",
    "important_X_test = selkbest.transform(important_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6955, 2000)\n",
      "(6955,)\n",
      "(2174, 2000)\n",
      "(2174,)\n",
      "(1739, 2000)\n",
      "(1739,)\n"
     ]
    }
   ],
   "source": [
    "print(important_X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(important_X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(important_X_cv.shape)\n",
    "print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                20          1          5000    0.012952  0.029261\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                20          1          7000    0.012952  0.029261\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                20          1         10000    0.012952  0.029261\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                20          1         12000    0.012952  0.029261\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                20          3          5000    0.014943  0.028552\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e7a0d879d7be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   n_jobs=-1 )\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mpredict_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpredict_y_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_child_weight= [ 20, 30, 50]\n",
    "max_depth= [1, 3, 5]\n",
    "n_estimators= [ 5000, 7000, 10000, 12000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw, \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                20          1          5000    0.015141  0.031138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-33447e092693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                    \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   n_jobs=-1 )\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mpredict_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpredict_y_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_child_weight= [ 20, 30, 50]\n",
    "max_depth= [1, 3, 5]\n",
    "n_estimators= [ 5000, 7000, 10000, 12000]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw, \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                   subsample=0.9,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss  cv_loss\n",
      "0                 3          5          5000    0.001944  0.02014\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 3          5          7000    0.001941  0.020116\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dcac4e1aa540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                    \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   n_jobs=-1 )\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mpredict_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpredict_y_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_child_weight= [ 3, 5, 10, 15]\n",
    "max_depth= [5]\n",
    "n_estimators= [ 5000, 7000, 10000, 12000]\n",
    "learning_rate = [0.3, 0.5, 0.7, 0.9]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "\n",
    "            xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                   min_child_weight=mcw, \n",
    "                                   max_depth=md, \n",
    "                                   n_estimators=ne,\n",
    "                                   subsample=0.8,\n",
    "                                  n_jobs=-1 )\n",
    "            xgbreg.fit(important_X_train, y_train)\n",
    "            predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "            predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "            train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "            cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "            ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "            print(ok)\n",
    "            xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with other samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          7000    0.000494  0.022349\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          7000    0.000494  0.022295\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1          7000    0.000486  0.024957\n",
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.000493  0.022331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-697645487a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                        \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       n_jobs=-1 )\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mpredict_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mpredict_y_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_X_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_child_weight= [ 1, 2, 3]\n",
    "max_depth= [1, 2, 3]\n",
    "n_estimators= [ 7000, 10000, 12000]\n",
    "learning_rate = [0.3, 0.5, 0.7]\n",
    "cl = ['min_child_weight', 'max_depth', 'n_estimators', 'train_loss', 'cv_loss']\n",
    "xgboostparam = pd.DataFrame(columns=cl)\n",
    "for mcw in min_child_weight:\n",
    "    for md in max_depth:\n",
    "        for ne in n_estimators:\n",
    "            for lr in learning_rate:\n",
    "                xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                       min_child_weight=mcw, \n",
    "                                       max_depth=md, \n",
    "                                       n_estimators=ne,\n",
    "                                       subsample=0.8,\n",
    "                                       learning_rate = lr,\n",
    "                                      n_jobs=-1 )\n",
    "                xgbreg.fit(important_X_train, y_train)\n",
    "                predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "                predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "                train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "                cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "                ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "                print(ok)\n",
    "                xgboostparam = xgboostparam.append(ok, ignore_index=True)\n",
    "\n",
    "# xgboostparam                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.001179  0.021203\n"
     ]
    }
   ],
   "source": [
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                       min_child_weight=2, \n",
    "                                       max_depth=3, \n",
    "                                       n_estimators=30000,\n",
    "                                       subsample=0.8,\n",
    "                                       learning_rate = 0.7,\n",
    "                                      n_jobs=-1 )\n",
    "xgbreg.fit(important_X_train, y_train)\n",
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001179342617424161\n",
      "0.01326506148774651\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_test = xgbreg.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=xgbreg.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.001179  0.021203\n"
     ]
    }
   ],
   "source": [
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                       min_child_weight=2, \n",
    "                                       max_depth=3, \n",
    "                                       n_estimators=30000,\n",
    "                                       subsample=0.8,\n",
    "                                       learning_rate = 0.7,\n",
    "                                      n_jobs=-1, importance_type=\"weight\" )\n",
    "xgbreg.fit(important_X_train, y_train)\n",
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001179342617424161\n",
      "0.01326506148774651\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_test = xgbreg.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=xgbreg.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.001151  0.017954\n"
     ]
    }
   ],
   "source": [
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                       min_child_weight=2, \n",
    "                                       max_depth=3, \n",
    "                                       n_estimators=30000,\n",
    "                                       subsample=0.8,\n",
    "                                       colsample_bytree =0.8,\n",
    "                                       learning_rate = 0.7,\n",
    "                                      n_jobs=-1, importance_type=\"weight\" )\n",
    "xgbreg.fit(important_X_train, y_train)\n",
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011505869209501628\n",
      "0.010148859501929626\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_test = xgbreg.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=xgbreg.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE CAN CONSIDER THIS MODEL AS IT IS MEETING THE REQUIREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.001144  0.019361\n"
     ]
    }
   ],
   "source": [
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                       min_child_weight=2, \n",
    "                                       max_depth=3, \n",
    "                                       n_estimators=30000,\n",
    "                                       subsample=0.8,\n",
    "                                       colsample_bytree =0.9,\n",
    "                                       learning_rate = 0.7,\n",
    "                                      n_jobs=-1, importance_type=\"weight\" )\n",
    "xgbreg.fit(important_X_train, y_train)\n",
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011444307642729944\n",
      "0.013716394717699995\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_test = xgbreg.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=xgbreg.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight  max_depth  n_estimators  train_loss   cv_loss\n",
      "0                 1          1         10000    0.001151  0.024881\n"
     ]
    }
   ],
   "source": [
    "xgbreg = XGBClassifier(nthread=-1,missing=None,seed=8, \n",
    "                                       min_child_weight=2, \n",
    "                                       max_depth=3, \n",
    "                                       n_estimators=30000,\n",
    "                                       subsample=0.8,\n",
    "                                       colsample_bytree =0.6,\n",
    "                                       learning_rate = 0.7,\n",
    "                                      n_jobs=-1, importance_type=\"weight\" )\n",
    "xgbreg.fit(important_X_train, y_train)\n",
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_cv = xgbreg.predict_proba(important_X_cv)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "cv_loss = log_loss(y_cv, predict_y_cv, labels=xgbreg.classes_, eps=1e-15)\n",
    "ok = pd.DataFrame([(mcw, md, ne, train_loss, cv_loss),], columns=cl)\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011509613165928698\n",
      "0.011056001719389181\n"
     ]
    }
   ],
   "source": [
    "predict_y_train = xgbreg.predict_proba(important_X_train)\n",
    "predict_y_test = xgbreg.predict_proba(important_X_test)\n",
    "\n",
    "train_loss = log_loss(y_train, predict_y_train, labels=xgbreg.classes_, eps=1e-15)\n",
    "test_loss = log_loss(y_test, predict_y_test, labels=xgbreg.classes_, eps=1e-15)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
